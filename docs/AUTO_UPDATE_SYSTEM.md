# Automatic Data Update System

## ğŸ”„ How Our Website Updates Data

Our system automatically keeps event data synchronized with the source websites (Eventbrite and City of Sydney) using a smart update mechanism.

---

## ğŸ“‹ Overview

### The System Has 3 Main Components:

1. **Cron Job Scheduler** (`jobs/scraperCron.js`)
   - Runs automatically every 6 hours
   - Scrapes latest data from all sources
   - No manual intervention needed

2. **Hash-Based Change Detection** (`services/hashService.js`)
   - Creates a unique fingerprint for each event
   - Detects when content changes (title, date, venue, etc.)
   - Only updates when actual changes occur

3. **Smart Upsert Service** (`services/upsertService.js`)
   - Insert new events
   - Update changed events
   - Mark missing events as inactive

---

## âš™ï¸ How It Works (Step by Step)

### Step 1: Scheduled Scraping
```
Every 6 hours (00:00, 06:00, 12:00, 18:00):
â”œâ”€â”€ Scrape Eventbrite
â”œâ”€â”€ Scrape City of Sydney
â””â”€â”€ Process all events
```

### Step 2: Event Processing

For **each scraped event**, the system:

```javascript
1. Check if event URL exists in database
   â”‚
   â”œâ”€ NO (New Event)
   â”‚   â”œâ”€ Generate hash from content
   â”‚   â”œâ”€ Save to database
   â”‚   â”œâ”€ Mark as "new"
   â”‚   â””â”€ Set lastScrapedAt to now
   â”‚
   â””â”€ YES (Existing Event)
       â”œâ”€ Generate hash from new content
       â”œâ”€ Compare with stored hash
       â”‚
       â”œâ”€ Hashes Different (Content Changed)
       â”‚   â”œâ”€ Update all fields
       â”‚   â”œâ”€ Mark as "updated"
       â”‚   â””â”€ Set lastScrapedAt to now
       â”‚
       â””â”€ Hashes Same (No Change)
           â””â”€ Only update lastScrapedAt
```

### Step 3: Cleanup Inactive Events

```javascript
After processing all events:
â”œâ”€ Find events not scraped in last 24 hours
â”œâ”€ Mark them as "inactive"
â””â”€ (They may have been removed from source website)
```

---

## ğŸ” Hash Generation (Change Detection)

### What is a Hash?
A hash is like a fingerprint for event data. If any important field changes, the hash changes.

### Fields Monitored for Changes:
- âœ… Title
- âœ… Description
- âœ… Date/Time
- âœ… Venue
- âœ… Address
- âœ… Image URL

### Example:
```javascript
Event A (First scrape):
{
  title: "Sydney Music Festival",
  venue: "Opera House",
  dateTime: "2026-03-15"
}
Hash: "a1b2c3d4e5f6..."

Event A (Second scrape - venue changed):
{
  title: "Sydney Music Festival",
  venue: "Town Hall",  // CHANGED
  dateTime: "2026-03-15"
}
Hash: "x9y8z7w6v5u4..."  // DIFFERENT HASH

System detects change â†’ Updates database
```

---

## ğŸ“Š Event Status Lifecycle

```
new â†’ updated â†’ inactive
 â”‚      â”‚         â”‚
 â”‚      â”‚         â””â”€ Not found in recent scrapes
 â”‚      â””â”€ Content changed from original
 â””â”€ First time scraped
```

---

## ğŸ• Cron Schedule Configuration

### Current Schedule: Every 6 Hours
```javascript
cron.schedule('0 */6 * * *', ...)
```

### Other Options:

| Schedule | Cron Expression | When It Runs |
|----------|----------------|--------------|
| Every hour | `'0 * * * *'` | :00 of every hour |
| Every 3 hours | `'0 */3 * * *'` | 00:00, 03:00, 06:00... |
| Every 12 hours | `'0 */12 * * *'` | 00:00, 12:00 |
| Daily at midnight | `'0 0 * * *'` | 00:00 every day |
| Every 5 minutes | `'*/5 * * * *'` | Good for testing |

### To Change Schedule:
Edit `backend/jobs/scraperCron.js` line 24:
```javascript
cron.schedule('YOUR_SCHEDULE_HERE', async () => {
  // ...
})
```

---

## ğŸ§ª Testing the Update System

### Method 1: Manual API Call
```bash
POST http://localhost:5000/api/scrape/all
```

### Method 2: Temporary Fast Schedule
Change cron to run every minute for testing:
```javascript
cron.schedule('* * * * *', async () => {
  // Runs every minute
})
```

---

## ğŸ“ˆ Monitoring Updates

### Console Logs Show:
```
ğŸ• Running scheduled scrape at: 2/2/2026, 12:00:00 PM
ğŸ“¥ Eventbrite: Found 25 events
âœ… Eventbrite: Inserted 3, Updated 2
ğŸ“¥ City of Sydney: Found 18 events
âœ… City of Sydney: Inserted 1, Updated 5
ğŸ—‘ï¸  Marked 4 events as inactive
âœ¨ Scrape complete: 4 new, 7 updated, 4 inactive
```

### Database Tracking:
Each event has:
- `lastScrapedAt` - When it was last found
- `status` - Current state (new/updated/inactive)
- `hash` - Content fingerprint
- `createdAt` - First time added
- `updatedAt` - Last modification

---

## ğŸ”§ Files Involved

```
backend/
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ scraperCron.js          # Automated scheduling
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ hashService.js          # Change detection
â”‚   â”œâ”€â”€ upsertService.js        # Insert/Update logic
â”‚   â””â”€â”€ scrapeContext.js        # Browser management
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ eventbrite/
â”‚   â”‚   â”œâ”€â”€ list.js             # Get event URLs
â”‚   â”‚   â”œâ”€â”€ detail.js           # Scrape event details
â”‚   â”‚   â””â”€â”€ index.js            # Main scraper
â”‚   â””â”€â”€ cityofsydney/
â”‚       â”œâ”€â”€ list.js
â”‚       â”œâ”€â”€ detail.js
â”‚       â””â”€â”€ index.js
â””â”€â”€ models/
    â””â”€â”€ Event.js                # Database schema
```

---

## ğŸ¯ Key Benefits

1. **Automatic Updates** - No manual work needed
2. **Smart Detection** - Only updates when content actually changes
3. **Data Integrity** - Tracks event history and status
4. **Performance** - Doesn't re-save identical data
5. **Monitoring** - Clear logs of all changes
6. **Cleanup** - Automatically identifies removed events

---

## ğŸš€ Starting the System

The cron job starts automatically when you run:
```bash
npm start
# or
npm run dev
```

You'll see:
```
â° Cron job scheduled: Event scraping every 6 hours
Server running on port 5000
MongoDB connected
```

The system is now monitoring and will update automatically!
